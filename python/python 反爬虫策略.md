---
title: python 反爬虫策略 
tags: python,爬虫
---
python 反爬虫策略

1. .限制IP地址单位时间的访问次数 ：
分析：没有哪个常人一秒钟内能访问相同网站5次，除非是程序访问，而有这种喜好的，就剩下搜索引擎爬虫和讨厌的采集器了。
弊端：一刀切，这同样会阻止搜索引擎对网站的收录
适用网站：不太依靠搜索引擎的网站
采集器会怎么做：减少单位时间的访问次数，减低采集效率
2. 屏蔽ip
分析：通过后台计数器，记录来访者ip和访问频率，人为分析来访记录，屏蔽可疑Ip。
弊端：似乎没什么弊端，就是站长忙了点
适用网站：所有网站，且站长能够知道哪些是google或者百度的机器人
采集器会怎么做：打游击战呗！利用ip代理采集一次换一次，不过会降低采集器的效率和网速(用代理嘛)。
3. 利用js加密网页内容
Note:这个方法我没接触过，只是从别处看来
分析：不用分析了，搜索引擎爬虫和采集器通杀
适用网站：极度讨厌搜索引擎和采集器的网站
采集器会这么做：你那么牛，都豁出去了，他就不来采你了
4. 网页里隐藏网站版权或者一些随机垃圾文字，这些文字风格写在css文件中
分析：虽然不能防止采集，但是会让采集后的内容充满了你网站的版权说明或者一些垃圾文字，因为一般采集器不会同时采集你的css文件，那些文字没了风格，就显示出来了。
适用网站：所有网站
采集器会怎么做：对于版权文字，好办，替换掉。对于随机的垃圾文字，没办法，勤快点了。
5. 用户登录才能访问网站内容 
分析：搜索引擎爬虫不会对每个这样类型的网站设计登录程序。听说采集器可以针对某个网站设计模拟用户登录提交表单行为。
适用网站：极度讨厌搜索引擎，且想阻止大部分采集器的网站
采集器会怎么做：制作拟用户登录提交表单行为的模块
6. 利用脚本语言做分页(隐藏分页)
分析：还是那句，搜索引擎爬虫不会针对各种网站的隐藏分页进行分析，这影响搜索引擎对其收录。但是，采集者在编写采集规则时，要分析目标网页代码，懂点脚本知识的人，就会知道分页的真实链接地址。
适用网站：对搜索引擎依赖度不高的网站，还有，采集你的人不懂脚本知识
采集器会怎么做：应该说采集者会怎么做，他反正都要分析你的网页代码，顺便分析你的分页脚本，花不了多少额外时间。
7. 防盗链措施 (只允许通过本站页面连接查看，如：Request.ServerVariables(“HTTP_REFERER“) )
分析：asp和php可以通过读取请求的HTTP_REFERER属性，来判断该请求是否来自本网站，从而来限制采集器，同样也限制了搜索引擎爬虫，严重影响搜索引擎对网站部分防盗链内容的收录。
适用网站：不太考虑搜索引擎收录的网站
采集器会怎么做：伪装HTTP_REFERER嘛，不难。
8. 全flash、图片或者pdf来呈现网站内容
分析：对搜索引擎爬虫和采集器支持性不好，这个很多懂点seo的人都知道
适用网站：媒体设计类并且不在意搜索引擎收录的网站
采集器会怎么做：不采了，走人
9. 网站随机采用不同模版
分析：因为采集器是根据网页结构来定位所需要的内容，一旦先后两次模版更换，采集规则就失效，不错。而且这样对搜索引擎爬虫没影响。
适用网站：动态网站，并且不考虑用户体验。
采集器会怎么做：一个网站模版不可能多于10个吧，每个模版弄一个规则就行了，不同模版采用不同采集规则。如果多于10个模版了，既然目标网站都那么费劲的更换模版，成全他，撤。
10. 采用动态不规则的html标签
分析：这个比较变态。考虑到html标签内含空格和不含空格效果是一样的，所以< div >和< div >对于页面显示效果一样，但是作为采集器的标记就是两个不同标记了。如果每次页面的html标签内空格数随机，那么
采集规则就失效了。但是，这对搜索引擎爬虫没多大影响。
适合网站：所有动态且不想遵守网页设计规范的网站。
采集器会怎么做：还是有对策的，现在html cleaner还是很多的，先清理了html标签，然后再写采集规则；应该用采集规则前先清理html标签，还是能够拿到所需数据

总结：
1. ip，账号频率，自动限速 得到延迟delay con 线程数
2. 设置 referer user - agent
3. js selenium splash 分析一些参数
4. 登录 cookie
5. 验证码，包括图像验证码、拖动验证码等等
# 爬虫技能书
![enter description here][1]


  [1]: ./images/%E7%88%AC%E8%99%AB%E6%8A%80%E8%83%BD%E6%A0%91_1.png "爬虫技能树"